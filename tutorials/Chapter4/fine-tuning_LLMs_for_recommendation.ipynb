{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tuning an LLM for Movie Recommendations\n",
        "\n",
        "#### Goals\n",
        "- Improve the model's relevance in generating recommendations.\n",
        "- Address challenges of domain-specific language and sparse user data.\n",
        "- Create a deployable recommendation pipeline.\n",
        "\n",
        "#### Requirements\n",
        "\n",
        "- A pre-trained LLM (e.g., GPT-3, GPT-4, or an open-source model like GPT-J).\n",
        "- A fine-tuning framework such as Hugging Face Transformers.\n",
        "- A movie dataset (e.g., MovieLens or IMDb).\n",
        "- Compute resources (GPU recommended)."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %pip install --upgrade datasets\n",
        "# %pip install datasets accelerate\n",
        "# %pip install protobuf==3.20.*\n",
        "# %pip install peft==0.10.0\n",
        "# %pip install transformers==4.37.2\n",
        "# %pip install --upgrade \"transformers>=4.37.0\"\n",
        "# %pip install rapidfuzz\n",
        "# %pip show peft transformers datasets"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1742904079153
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "import os\n",
        "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
        "\n",
        "# Read the YAML file\n",
        "with open('./../../../Curify/curify_api.yaml', 'r') as yaml_file:\n",
        "    data = yaml.safe_load(yaml_file)"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {
        "gather": {
          "logged": 1742904079287
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 1: Preprocess Movielens Data and Format Input-Output Pairs\n",
        "\n",
        "Use a dataset containing:\n",
        "\n",
        "- Movie descriptions (e.g., summaries, genres, metadata).\n",
        "- User-item interaction data (e.g., ratings, reviews, watch history).\n",
        "\n",
        "Input-Output Pair Formatting:\n",
        "\n",
        "- Input: A prompt with user context, such as:\n",
        "\n",
        "`User preferences: [list of liked movies]. Recommend 3 movies based on their taste.`\n",
        "\n",
        "- Output: A list of relevant recommendations or a response explaining the recommendations."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define download URL and target directory\n",
        "# url=\"https://files.grouplens.org/datasets/movielens/ml-10m.zip\"\n",
        "# movielens_zip=\"./../Data/movielens.zip\"\n",
        "# target_dir=\"./../Data/\"\n",
        "\n",
        "# # Ensure the target directory exists\n",
        "# mkdir -p $target_dir\n",
        "\n",
        "# # Download the dataset\n",
        "# wget -q $url -O $movielens_zip\n",
        "\n",
        "# # Unzip the dataset to the target directory\n",
        "# unzip -q $movielens_zip -d $target_dir\n",
        "\n",
        "# echo \"Dataset downloaded and extracted to $target_dir\""
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1742904079541
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_finetuning_data(data, user_id, n_history=5, n_target=3):\n",
        "    \"\"\"\n",
        "    Prepares prompt-response pairs for fine-tuning a movie recommendation LLM.\n",
        "    - Uses a few liked/disliked movies as history.\n",
        "    - Predicts the next liked movies.\n",
        "    \"\"\"\n",
        "    user_data = data[data['userId'] == user_id].sort_values(by='timestamp')\n",
        "\n",
        "    # Ensure enough data for history + target\n",
        "    if len(user_data) < (n_history + n_target):\n",
        "        return None\n",
        "\n",
        "    # Split: last n_history interactions for history, rest is future\n",
        "    past = user_data.iloc[:n_history]\n",
        "    future = user_data.iloc[n_history:]\n",
        "\n",
        "    liked = past[past['rating'] >= 4]['title'].tolist()\n",
        "    disliked = past[past['rating'] <= 2]['title'].tolist()\n",
        "    targets = future[future['rating'] >= 4]\n",
        "\n",
        "    if targets.empty:\n",
        "        return None\n",
        "\n",
        "    # Prompt construction\n",
        "    parts = [\"You are a helpful movie recommendation assistant.\"]\n",
        "    if liked:\n",
        "        parts.append(\"The user liked: \" + \", \".join(liked) + \".\")\n",
        "    if disliked:\n",
        "        parts.append(\"The user disliked: \" + \", \".join(disliked) + \".\")\n",
        "\n",
        "    parts.append(\"Recommend new movies the user might enjoy next. Don't repeat any from the history. Output only movie titles separated by semicolons (`;`).\")\n",
        "\n",
        "    prompt = \" \".join(parts)\n",
        "    response = \"; \".join(targets['title'].tolist())\n",
        "\n",
        "    return {\n",
        "        \"user_id\": user_id,\n",
        "        \"prompt\": prompt,\n",
        "        \"history\": parts[1:-1],  # optional: useful for debugging\n",
        "        \"response\": response,\n",
        "        \"liked_movies\": targets['movieId'].tolist()\n",
        "    }\n"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1742904079653
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 2: LoRA (Parameter-Efficient Fine-Tuning)\n",
        "\n",
        "LoRA requires fewer resources by updating a subset of parameters.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "import time\n",
        "\n",
        "def train_lora_model_olmo(train_samples, output_dir=\"./lora_finetuned_model\", eval_samples=None, base_model_name=\"amd/AMD-OLMo-1B-SFT\"):\n",
        "    \"\"\"\n",
        "    Train a LoRA-adapted OLMo causal LM model on prompt-response movie recommendation data.\n",
        "\n",
        "    Args:\n",
        "        train_samples (list): List of samples, each with 'prompt', 'response'.\n",
        "        output_dir (str): Directory to save the model.\n",
        "        eval_samples (list): Optional evaluation samples.\n",
        "        base_model_name (str): Hugging Face model ID.\n",
        "\n",
        "    Returns:\n",
        "        model: The trained model.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load tokenizer and model\n",
        "        tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "        model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
        "\n",
        "        # Apply LoRA configuration\n",
        "        lora_config = LoraConfig(\n",
        "            task_type=TaskType.CAUSAL_LM,\n",
        "            r=8,\n",
        "            lora_alpha=16,\n",
        "            lora_dropout=0.1,\n",
        "            target_modules=[\"q_proj\", \"v_proj\"]  # Adjust if needed\n",
        "        )\n",
        "        model = get_peft_model(model, lora_config)\n",
        "\n",
        "        # Tokenization function (causal LM style)\n",
        "        def tokenize_function(example):\n",
        "            full_text = example[\"prompt\"] + \"\\n\" + example[\"response\"]\n",
        "            tokenized = tokenizer(\n",
        "                full_text,\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "                max_length=512,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "            tokenized = {k: v[0] for k, v in tokenized.items()}  # Convert to non-batch\n",
        "            tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
        "            return tokenized\n",
        "\n",
        "        # Convert to HuggingFace Dataset\n",
        "        train_dataset = Dataset.from_list(train_samples)\n",
        "        train_dataset = train_dataset.map(tokenize_function, remove_columns=[\"prompt\", \"history\", \"response\"])\n",
        "\n",
        "        if eval_samples:\n",
        "            eval_dataset = Dataset.from_list(eval_samples)\n",
        "            eval_dataset = eval_dataset.map(tokenize_function, remove_columns=[\"prompt\", \"history\", \"response\"])\n",
        "        else:\n",
        "            split = train_dataset.train_test_split(test_size=0.1)\n",
        "            train_dataset = split[\"train\"]\n",
        "            eval_dataset = split[\"test\"]\n",
        "\n",
        "        # Collator for Causal LM\n",
        "        data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "        # Training arguments\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "            per_device_train_batch_size=4,\n",
        "            per_device_eval_batch_size=4,\n",
        "            num_train_epochs=3,\n",
        "            evaluation_strategy=\"epoch\",\n",
        "            save_strategy=\"epoch\",\n",
        "            logging_dir=\"./logs\",\n",
        "            save_total_limit=2,\n",
        "            report_to=\"none\"\n",
        "        )\n",
        "\n",
        "        # Trainer\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=eval_dataset,\n",
        "            tokenizer=tokenizer,\n",
        "            data_collator=data_collator\n",
        "        )\n",
        "\n",
        "        # Training loop\n",
        "        start_time = time.time()\n",
        "        trainer.train()\n",
        "        print(f\"Training completed in {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "        trainer.save_model(output_dir)\n",
        "        print(f\"Model saved to {output_dir}\")\n",
        "\n",
        "        return model\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during training: {e}\")\n",
        "        return False\n"
      ],
      "outputs": [],
      "execution_count": 24,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1742904079776
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 3: Analyze and Compare\n",
        "\n",
        "- Compare methods using metrics like precision@K, recall@K.\n",
        "- training 100 examples: 293.55438566207886"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from rapidfuzz import process\n",
        "from sklearn.metrics import ndcg_score\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "def map_title_to_id(title, movie_lookup, threshold=80):\n",
        "    result = process.extractOne(title, movie_lookup.keys())\n",
        "    if result is None:\n",
        "        return None\n",
        "    match, score, _ = result  # Correct unpacking for rapidfuzz\n",
        "    return movie_lookup[match] if score >= threshold else None\n",
        "\n",
        "def build_prompt_with_few_shots(few_shot_examples, target_history, num_recommend=20):\n",
        "    \"\"\"\n",
        "    Constructs a prompt with few-shot examples and target user history.\n",
        "    \"\"\"\n",
        "    prompt = \"You are a movie recommendation assistant.\\n\\n\"\n",
        "    prompt += \"Here are a few examples:\\n\\n\"\n",
        "\n",
        "    # Add few-shot examples\n",
        "    for example in few_shot_examples:\n",
        "        prompt += f\"{example['history']}\"\n",
        "        prompt += \"Recommended movies: \" + example[\"response\"] + \"\\n\\n\"\n",
        "\n",
        "    # Add target history and instruction\n",
        "    prompt += f\"{target_history}\"\n",
        "    prompt += f\"Based on this history, recommend exactly {num_recommend} movies the user will like. \\n\\n\"\n",
        "    prompt += \"Do NOT repeat any movies from the user's history.\\n\"\n",
        "    prompt += \"Output only the movie titles, separated by semicolons `;`, ordered by relevance.\"\n",
        "\n",
        "    return prompt\n",
        "\n",
        "def extract_recommended_titles(generated_text, num_recommend=20):\n",
        "    \"\"\"\n",
        "    Extract movie titles from generated LLM output with robustness to varied separators and formats.\n",
        "    Handles:\n",
        "      - Delimiters: newline (\\n), semicolon (;), comma (,)\n",
        "      - Missing parentheses around year: e.g., 'Movie Title 1998' â†’ 'Movie Title (1998)'\n",
        "    \"\"\"\n",
        "    # Step 1: Cut text after 'Recommended movies:'\n",
        "    match = re.search(r\"(Recommended movies:|recommend the following movies:)\", generated_text, re.IGNORECASE)\n",
        "    text_after = generated_text[match.end():].strip() if match else generated_text.strip()\n",
        "\n",
        "    # Step 2: Normalize delimiters â†’ replace \\n and semicolons with commas\n",
        "    text_after = text_after.replace(\"\\n\", \",\").replace(\";\", \",\")\n",
        "\n",
        "    # Step 3: Split by comma, clean whitespace\n",
        "    raw_titles = [t.strip() for t in text_after.split(\",\") if t.strip()]\n",
        "\n",
        "    # Step 4: Clean titles and fix missing parentheses for year\n",
        "    cleaned_titles = []\n",
        "    for title in raw_titles:\n",
        "        # Match ending year (e.g., \"Movie Title 1998\")\n",
        "        match = re.match(r\"^(.*?)(?:\\s+(\\d{4}))?$\", title)\n",
        "        if match:\n",
        "            title_name = match.group(1).strip()\n",
        "            year = match.group(2)\n",
        "            if year and not title_name.endswith(f\"({year})\"):\n",
        "                title_name = f\"{title_name} ({year})\"\n",
        "            cleaned_titles.append(title_name)\n",
        "\n",
        "    # Step 5: Deduplicate while preserving order\n",
        "    seen = set()\n",
        "    final_titles = []\n",
        "    for title in cleaned_titles:\n",
        "        if title not in seen:\n",
        "            final_titles.append(title)\n",
        "            seen.add(title)\n",
        "\n",
        "    return final_titles[:num_recommend]\n",
        "\n",
        "def generate_recommendations(model, movie_lookup, eval_samples, num_recommend=20, is_fine_tuned=True, few_shot_samples=None):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model.config._name_or_path)\n",
        "    user_recommendations = {}\n",
        "\n",
        "    for sample in eval_samples:\n",
        "        # Build prompt\n",
        "        if few_shot_samples:\n",
        "            input_text = build_prompt_with_few_shots(few_shot_samples, sample['history'], num_recommend)\n",
        "        else:\n",
        "            input_text = sample['prompt']  + f\"\\nRecommended movies:\"\n",
        "\n",
        "        # Generate\n",
        "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "        outputs = model.generate(**inputs, max_new_tokens=200, do_sample=True, top_k=50, top_p=0.95)\n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Extract titles\n",
        "        generated_titles = extract_recommended_titles(generated_text, num_recommend)\n",
        "\n",
        "        print(\"\\n[INPUT PROMPT]\\n\", input_text)\n",
        "        print(\"[GENERATED TEXT]\\n\", generated_text)\n",
        "        print(\"[GENERATED TITLES]\\n\", generated_titles)\n",
        "\n",
        "        # Fuzzy match to movie IDs\n",
        "        recommended_movies = []\n",
        "        for title in generated_titles:\n",
        "            movie_id = map_title_to_id(title, movie_lookup)\n",
        "            if movie_id:\n",
        "                recommended_movies.append(movie_id)\n",
        "\n",
        "        user_recommendations[sample[\"user_id\"]] = recommended_movies\n",
        "\n",
        "    return user_recommendations\n",
        "\n",
        "# Step 2: Evaluate Recommendations at Multiple k's\n",
        "def evaluate_recommendations(user_recommendations, eval_samples, movie_lookup, ks=[5, 10, 20]):\n",
        "    results = {k: {\"precision\": [], \"recall\": [], \"ndcg\": []} for k in ks}\n",
        "\n",
        "    for sample in eval_samples:\n",
        "        user_id = sample[\"user_id\"]\n",
        "        if user_id not in user_recommendations:\n",
        "            continue\n",
        "\n",
        "        recommended_movies = user_recommendations[user_id]\n",
        "        ground_truth_movies = sample[\"liked_movies\"]\n",
        "\n",
        "        if not ground_truth_movies:\n",
        "            continue\n",
        "\n",
        "        for k in ks:\n",
        "            rec_at_k = recommended_movies[:k]\n",
        "            relevance = [1 if movie in ground_truth_movies else 0 for movie in rec_at_k]\n",
        "\n",
        "            precision = sum(relevance) / k\n",
        "            recall = sum(relevance) / len(ground_truth_movies)\n",
        "\n",
        "            predicted_scores = list(range(k, 0, -1))\n",
        "            try:\n",
        "                ndcg = ndcg_score([relevance], [predicted_scores], k=k)\n",
        "            except:\n",
        "                ndcg = 0.0\n",
        "\n",
        "            results[k][\"precision\"].append(precision)\n",
        "            results[k][\"recall\"].append(recall)\n",
        "            results[k][\"ndcg\"].append(ndcg)\n",
        "\n",
        "    # Aggregate averages\n",
        "    aggregated = {\n",
        "        f\"@{k}\": {\n",
        "            \"Precision\": round(np.mean(results[k][\"precision\"]), 4),\n",
        "            \"Recall\": round(np.mean(results[k][\"recall\"]), 4),\n",
        "            \"NDCG\": round(np.mean(results[k][\"ndcg\"]), 4)\n",
        "        }\n",
        "        for k in ks\n",
        "    }\n",
        "\n",
        "    return aggregated\n"
      ],
      "outputs": [],
      "execution_count": 25,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1742904079904
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "\n",
        "movielens_folder = \"./../Data/ml-1m/\"\n",
        "\n",
        "# Read ratings.dat with the correct delimiter and encoding\n",
        "ratings = pd.read_csv(\n",
        "    f\"{movielens_folder}ratings.dat\", \n",
        "    sep=\"::\", \n",
        "    engine=\"python\", \n",
        "    header=None, \n",
        "    names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"], \n",
        "    encoding=\"ISO-8859-1\"\n",
        ")\n",
        "\n",
        "# Read movies.dat with the correct encoding\n",
        "movies = pd.read_csv(\n",
        "    f\"{movielens_folder}movies.dat\", \n",
        "    sep=\"::\", \n",
        "    engine=\"python\", \n",
        "    header=None, \n",
        "    names=[\"movieId\", \"title\", \"genres\"], \n",
        "    encoding=\"ISO-8859-1\"\n",
        ")\n",
        "\n",
        "movie_lookup = movies.set_index('title')['movieId'].to_dict()\n",
        "\n",
        "# Merge ratings with movie metadata\n",
        "data = ratings.merge(movies, on=\"movieId\")\n",
        "\n",
        "# Convert timestamp to datetime format\n",
        "data[\"timestamp\"] = pd.to_datetime(data[\"timestamp\"], unit=\"s\")\n",
        "\n",
        "# Example usage\n",
        "user_samples = [prepare_finetuning_data(data, user_id) for user_id in data['userId'].unique()]\n",
        "user_samples = [sample for sample in user_samples if sample]\n",
        "random.shuffle(user_samples)\n",
        "\n",
        "# Define number of samples\n",
        "n_train = 500\n",
        "n_test = 300\n",
        "\n",
        "# Split into train and eval\n",
        "train_samples = user_samples[:n_train]\n",
        "eval_samples = user_samples[n_train:n_train + n_test]\n",
        "\n",
        "fewshot_samples = random.sample(train_samples, 5)\n",
        "\n",
        "# Track fine-tuning time\n",
        "start_finetune = time.time()\n",
        "fine_tuned_model = train_lora_model_olmo(train_samples, eval_samples=eval_samples)\n",
        "end_finetune = time.time()\n",
        "finetune_time = end_finetune - start_finetune\n",
        "\n",
        "# Load the non-fine-tuned model\n",
        "non_fine_tuned_model = AutoModelForCausalLM.from_pretrained(\"amd/AMD-OLMo-1B-SFT\")\n",
        "\n",
        "# Track evaluation time\n",
        "start_eval = time.time()\n",
        "reco_FT = generate_recommendations(fine_tuned_model, movie_lookup, eval_samples, is_fine_tuned=True)\n",
        "reco_zeroshot = generate_recommendations(non_fine_tuned_model, movie_lookup, eval_samples, is_fine_tuned=False)\n",
        "reco_fewshot = generate_recommendations(non_fine_tuned_model, movie_lookup, eval_samples, is_fine_tuned=False, few_shot_samples=fewshot_samples)\n",
        "\n",
        "eval_FT = evaluate_recommendations(reco_FT, eval_samples, movie_lookup)\n",
        "eval_zeroshot = evaluate_recommendations(reco_zeroshot, eval_samples, movie_lookup)\n",
        "eval_fewshot = evaluate_recommendations(reco_fewshot, eval_samples, movie_lookup)\n",
        "end_eval = time.time()\n",
        "eval_time = end_eval - start_eval\n",
        "\n",
        "# Serialize results with time metrics\n",
        "results_combined = {\n",
        "    \"Fine-tuning\": {\n",
        "        \"metrics\": eval_FT,\n",
        "        \"finetune_time_seconds\": finetune_time\n",
        "    },\n",
        "    \"Zeroshot\": {\n",
        "        \"metrics\": eval_zeroshot\n",
        "    },\n",
        "    \"Fewshot\": {\n",
        "        \"metrics\": eval_fewshot\n",
        "    },\n",
        "    \"Evaluation_time_seconds\": eval_time\n",
        "}\n",
        "# Save to JSON file\n",
        "with open(\"fine_tuning.json\", \"w\") as f:\n",
        "    json.dump(results_combined, f, indent=2, default=str)\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/500 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9834643101ac46c9898c882a00ddf177"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/300 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fbc3a6133f414ec0ab98f13ab5afb3b4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py38/lib/python3.8/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/cuda/__init__.py:654: UserWarning: Can't initialize NVML\n  warnings.warn(\"Can't initialize NVML\")\n/tmp/ipykernel_52299/700942001.py:84: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='148' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [148/375 3:58:42 < 6:11:08, 0.01 it/s, Epoch 1.18/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>1.246373</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 26,
      "metadata": {
        "gather": {
          "logged": 1742902417409
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_combined"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1742902437874
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(1)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(1)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(1)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "1\n"
        }
      ],
      "execution_count": 33,
      "metadata": {
        "gather": {
          "logged": 1743040169909
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernel_info": {
      "name": "python38-azureml"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}