{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Pairwise Preference Distillation with Flan-T5\n",
        "\n",
        "#### Objective\n",
        "- Understand knowledge distillation and its application to recommendation systems.\n",
        "- Distill a large LLM (teacher) into a smaller, more efficient model (student) for recommendation tasks.\n",
        "- Evaluate the performance and efficiency of the distilled model compared to the teacher model.\n",
        "\n",
        "\n",
        "#### Prerequisite: Install Necessary Libraries"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %pip install transformers datasets evaluate sentencepiece accelerate openai"
      ],
      "outputs": [],
      "execution_count": 25,
      "metadata": {
        "gather": {
          "logged": 1752453785989
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "print(openai.__version__)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "1.93.2\n"
        }
      ],
      "execution_count": 26,
      "metadata": {
        "gather": {
          "logged": 1752453786186
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 1: Prepare the Dataset from MovieLens Data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "def generate_pairwise_prompts_from_movielens(movielens_folder=\"./../Data/ml-1m/\", num_pairs_per_user=5, use_cot =False):\n",
        "    \"\"\"\n",
        "    Generate pairwise comparison prompts from MovieLens data.\n",
        "    \n",
        "    Args:\n",
        "        movielens_folder (str): Path to MovieLens dataset folder.\n",
        "        num_pairs_per_user (int): Number of prompts to generate per user.\n",
        "    \n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame containing prompts and labels.\n",
        "    \"\"\"\n",
        "    # Read ratings and movies data\n",
        "    ratings = pd.read_csv(\n",
        "        f\"{movielens_folder}ratings.dat\", \n",
        "        sep=\"::\", \n",
        "        engine=\"python\", \n",
        "        header=None, \n",
        "        names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"], \n",
        "        encoding=\"ISO-8859-1\"\n",
        "    )\n",
        "\n",
        "    movies = pd.read_csv(\n",
        "        f\"{movielens_folder}movies.dat\", \n",
        "        sep=\"::\", \n",
        "        engine=\"python\", \n",
        "        header=None, \n",
        "        names=[\"movieId\", \"title\", \"genres\"], \n",
        "        encoding=\"ISO-8859-1\"\n",
        "    )\n",
        "\n",
        "    # Merge ratings with movie info\n",
        "    movie_ratings = pd.merge(ratings, movies, on='movieId')\n",
        "\n",
        "    def generate_prompt_label_pairs(user_id, user_rated_movies, all_movies, n_pairs=5, use_cot=False):\n",
        "        import random\n",
        "        pairs = []\n",
        "\n",
        "        liked_movies = user_rated_movies[user_rated_movies['rating'] >= 4]\n",
        "        disliked_movies = user_rated_movies[user_rated_movies['rating'] <= 2.5]\n",
        "\n",
        "        if len(liked_movies) < 4:\n",
        "            return []\n",
        "\n",
        "        for _ in range(n_pairs):\n",
        "            n = min(len(liked_movies), 11)\n",
        "            liked_sample = liked_movies.sample(n).reset_index(drop=True)\n",
        "\n",
        "            context_sample = liked_sample.iloc[:-1]\n",
        "            prediction_movie = liked_sample.iloc[-1]\n",
        "            context_sample = context_sample.sample(min(len(context_sample), 10))\n",
        "\n",
        "            context_movies = [\n",
        "                f\"{row['title']} ({row['genres']})\"\n",
        "                for _, row in context_sample.iterrows()\n",
        "            ]\n",
        "\n",
        "            if len(disliked_movies) > 0:\n",
        "                disliked = disliked_movies.sample(1).iloc[0]\n",
        "            else:\n",
        "                unrated = all_movies[~all_movies['movieId'].isin(user_rated_movies['movieId'])]\n",
        "                if len(unrated) == 0:\n",
        "                    continue\n",
        "                disliked = unrated.sample(1).iloc[0]\n",
        "\n",
        "            if random.random() < 0.5:\n",
        "                movie1, movie2 = prediction_movie, disliked\n",
        "                correct_label = 1\n",
        "            else:\n",
        "                movie1, movie2 = disliked, prediction_movie\n",
        "                correct_label = 2\n",
        "\n",
        "            intro = f\"The user liked the following movies: {', '.join(context_movies)}.\\n\\n\"\n",
        "            cot_instruction = (\n",
        "                \"Please think step-by-step about the genre and the year of each movie when making a decision.\\n\\n\"\n",
        "                if use_cot else \"\"\n",
        "            )\n",
        "            question = f\"\"\"Which movie is the user more likely to prefer?\n",
        "                1. {movie1['title']} ({movie1['genres']})\n",
        "                2. {movie2['title']} ({movie2['genres']})\n",
        "                Please answer with 1 or 2 only.\"\"\"\n",
        "\n",
        "            prompt = intro + cot_instruction + question\n",
        "            pairs.append((prompt, correct_label))\n",
        "\n",
        "        return pairs\n",
        "\n",
        "    # Process all users\n",
        "    all_pairs = []\n",
        "    user_groups = movie_ratings.groupby('userId')\n",
        "\n",
        "    for user_id, user_rated_movies in user_groups:\n",
        "        pairs = generate_prompt_label_pairs(user_id, user_rated_movies, movies, num_pairs_per_user, use_cot= use_cot)\n",
        "        all_pairs.extend(pairs)\n",
        "    \n",
        "    df = pd.DataFrame(all_pairs, columns=['prompt', 'label'])\n",
        "    return df\n"
      ],
      "outputs": [],
      "execution_count": 27,
      "metadata": {
        "gather": {
          "logged": 1752453786371
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 2: Prepare Tokenizer and Load the Teacher and Student Model"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "# import mlflow\n",
        "# mlflow.autolog(disable=True)\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class ComparisonDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.examples = data.reset_index(drop=True)  # Ensure integer index\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.examples.iloc[idx]  # âœ… Access DataFrame row correctly\n",
        "        prompt = row[\"prompt\"]\n",
        "        label = int(row[\"label\"])  # Ensure label is integer\n",
        "\n",
        "        inputs = self.tokenizer(\n",
        "            prompt,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=128,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
        "            \"label\": label\n",
        "        }\n"
      ],
      "outputs": [],
      "execution_count": 28,
      "metadata": {
        "gather": {
          "logged": 1752453786531
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 4: Distill Knowledge From Teacher to Student"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# ============ 5. Define Logits-Based Distillation Loss ============\n",
        "def distillation_loss(student_logits, teacher_logits, temperature=2.0):\n",
        "    student_log_probs = torch.nn.functional.log_softmax(student_logits / temperature, dim=-1)\n",
        "    teacher_probs = torch.nn.functional.softmax(teacher_logits / temperature, dim=-1)\n",
        "    loss = torch.nn.functional.kl_div(\n",
        "        student_log_probs,\n",
        "        teacher_probs,\n",
        "        reduction='batchmean'\n",
        "    ) * (temperature ** 2)\n",
        "    return loss\n",
        "\n",
        "# ============ 6. Distillation Training Loop ============\n",
        "def train_distill_model(student, teacher, dataset, tokenizer, device=\"cpu\", loss_type=\"kl\", temperature=2.0):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        loss_type: 'kl' for KL divergence only, 'hybrid' for KL + CE loss\n",
        "    \"\"\"\n",
        "    student.train()\n",
        "    teacher.eval()\n",
        "\n",
        "    optimizer = torch.optim.Adam(student.parameters(), lr=5e-5)\n",
        "\n",
        "    for epoch in range(3):\n",
        "        total_loss = 0\n",
        "        for sample in dataset:\n",
        "            input_ids = sample['input_ids'].unsqueeze(0).to(device)\n",
        "            attention_mask = sample['attention_mask'].unsqueeze(0).to(device)\n",
        "            \n",
        "            label_text = str(sample[\"label\"])\n",
        "            label_tokenized = tokenizer(label_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "            labels = label_tokenized[\"input_ids\"].to(device)\n",
        "            decoder_input_ids = student.prepare_decoder_input_ids_from_labels(labels)\n",
        "            # labels = torch.tensor([sample['label']], dtype=torch.long).to(device)\n",
        "\n",
        "            # decoder_input_ids = student.prepare_decoder_input_ids_from_labels(labels)\n",
        "\n",
        "            # Forward pass for teacher\n",
        "            with torch.no_grad():\n",
        "                teacher_outputs = teacher(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    decoder_input_ids=decoder_input_ids\n",
        "                )\n",
        "                teacher_logits = teacher_outputs.logits\n",
        "\n",
        "            # Forward pass for student\n",
        "            student_outputs = student(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                decoder_input_ids=decoder_input_ids,\n",
        "                labels=labels if loss_type == \"hybrid\" else None\n",
        "            )\n",
        "            student_logits = student_outputs.logits\n",
        "\n",
        "            # Compute loss\n",
        "            if loss_type == \"kl\":\n",
        "                loss = distillation_loss(student_logits, teacher_logits, temperature=temperature)\n",
        "            elif loss_type == \"hybrid\":\n",
        "                ce_loss = student_outputs.loss\n",
        "                kl_loss = distillation_loss(student_logits, teacher_logits, temperature=temperature)\n",
        "                loss = ce_loss + 0.5 * kl_loss\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported loss_type: {loss_type}\")\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: {loss_type.upper()} distillation loss = {total_loss:.4f}\")\n"
      ],
      "outputs": [],
      "execution_count": 29,
      "metadata": {
        "gather": {
          "logged": 1752453786685
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 7: Evaluate the Distilled Model\n",
        "- Evaluate model accuracy of teacher and distilled models.\n",
        "- Compare inference time of teacher and distilled models."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "import openai\n",
        "\n",
        "with open('./../../../Curify/curify_api.yaml', 'r') as yaml_file:\n",
        "    data = yaml.safe_load(yaml_file)\n",
        "\n",
        "# Access the API keys and other configuration data\n",
        "openai_api_key = data.get('openai').get('api_key')\n",
        "\n",
        "client = openai.OpenAI(api_key=openai_api_key)\n",
        "\n",
        "\n",
        "def filter_hard_samples_with_gpt4o(df, model=\"gpt-4o-mini\", sleep_time=0.5):\n",
        "    \"\"\"\n",
        "    Filter out evaluation samples that GPT-4o answers incorrectly.\n",
        "    Returns the filtered dataframe and the percentage removed.\n",
        "    \"\"\"\n",
        "    import time\n",
        "\n",
        "    filtered = []\n",
        "    retained = []\n",
        "\n",
        "    for i, row in df.iterrows():\n",
        "        prompt = row['prompt']\n",
        "        label = str(row['label']).strip()\n",
        "\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant that chooses between two movie options.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                temperature=0.2\n",
        "            )\n",
        "            \n",
        "            gpt_output = response.choices[0].message.content.strip()\n",
        "            print(gpt_output)\n",
        "            if label in gpt_output:\n",
        "                retained.append(row)\n",
        "            else:\n",
        "                filtered.append(row)\n",
        "\n",
        "            time.sleep(sleep_time)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error on row {i}: {e}\")\n",
        "            retained.append(row)  # Fail-safe: keep sample\n",
        "\n",
        "    retained_df = pd.DataFrame(retained)\n",
        "    filtered_pct = len(filtered) / len(df) * 100\n",
        "\n",
        "    print(f\"Filtered {len(filtered)} out of {len(df)} samples ({filtered_pct:.2f}%) using GPT-4o.\")\n",
        "\n",
        "    return retained_df, filtered_pct\n",
        "\n",
        "\n",
        "def evaluate_model(model, dataframe, tokenizer):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    results = []\n",
        "    \n",
        "    for _, row in dataframe.iterrows():\n",
        "        prompt = row['prompt']\n",
        "        label = str(row['label']).strip()\n",
        "        inputs = tokenizer(prompt, return_tensors='pt')\n",
        "        outputs = model.generate(**inputs, max_new_tokens=10)\n",
        "        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "        \n",
        "        results.append({\n",
        "            \"prompt\": prompt,\n",
        "            \"label\": label,\n",
        "            \"decoded\": decoded\n",
        "        })\n",
        "\n",
        "        if label in decoded:\n",
        "            correct += 1\n",
        "\n",
        "    accuracy = correct / len(dataframe)\n",
        "    return results, accuracy\n",
        "    "
      ],
      "outputs": [],
      "execution_count": 30,
      "metadata": {
        "gather": {
          "logged": 1752453786827
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from datasets import Dataset\n",
        "\n",
        "def run_distillation_experiment(train_data, test_data, use_cot=True, loss_type=\"kl\"):\n",
        "    print(f\"\\n========== Running Experiment (use_cot={use_cot}, loss_type='{loss_type}') ==========\")\n",
        "\n",
        "    # Tokenizer and datasets\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
        "    train_dataset = ComparisonDataset(train_data, tokenizer)\n",
        "    test_dataset = ComparisonDataset(test_data, tokenizer)\n",
        "\n",
        "    # Load models\n",
        "    teacher = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")\n",
        "    student = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
        "\n",
        "    # Evaluation before distillation\n",
        "    print(\"\\nBefore distillation:\")\n",
        "    start_time = time.time()\n",
        "    results_large, acc_large = evaluate_model(teacher, test_data, tokenizer)\n",
        "    inference_time_large = time.time() - start_time\n",
        "\n",
        "    start_time = time.time()\n",
        "    results_small, acc_small = evaluate_model(student, test_data, tokenizer)\n",
        "    inference_time_small = time.time() - start_time\n",
        "\n",
        "    print(f\"Flan-T5-Large Accuracy: {acc_large:.2f}\")\n",
        "    print(f\"Flan-T5-Small Accuracy: {acc_small:.2f}\")\n",
        "\n",
        "    # Distillation\n",
        "    print(\"\\nTraining distilled student...\")\n",
        "    start_time = time.time()\n",
        "    train_distill_model(student, teacher, train_dataset, tokenizer, loss_type=loss_type)\n",
        "    distillation_time = time.time() - start_time\n",
        "    print(f\"Distillation completed in {distillation_time:.2f} seconds.\")\n",
        "\n",
        "    # Evaluation after distillation\n",
        "    results_distilled, acc_distilled = evaluate_model(student, test_data, tokenizer)\n",
        "    print(f\"Distilled Flan-T5-Small Accuracy: {acc_distilled:.2f}\")\n",
        "\n",
        "    evaluation_final = {\n",
        "        \"acc_large\": acc_large,\n",
        "        \"acc_small\": acc_small,\n",
        "        \"acc_distilled\": acc_distilled,\n",
        "        \"time_large\": inference_time_large,\n",
        "        \"time_small\": inference_time_small,\n",
        "        \"distillation_time\": distillation_time,\n",
        "        \"Inference_volume\": len(test_data)\n",
        "    }\n",
        "    raw_outputs = {\n",
        "        \"large\": results_large,\n",
        "        \"small_before_distillation\": results_small,\n",
        "        \"small_after_distillation\": results_distilled\n",
        "    }\n",
        "    return evaluation_final, raw_outputs\n"
      ],
      "outputs": [],
      "execution_count": 31,
      "metadata": {
        "gather": {
          "logged": 1752453786963
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Global Preparation\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 32,
      "metadata": {
        "gather": {
          "logged": 1752453787099
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "evaluation_res = {}\n",
        "raw_res = {}\n",
        "\n",
        "for use_cot in [False, True]:\n",
        "    # Generate and filter once\n",
        "    prompt_data_full = generate_pairwise_prompts_from_movielens(use_cot=use_cot)  # always generate CoT version\n",
        "\n",
        "    prompt_data = prompt_data_full.head(5000)\n",
        "    filtered_data, pct_filtered = filter_hard_samples_with_gpt4o(prompt_data)\n",
        "    print(f\"Filtered dataset size: {len(filtered_data)} (Filtered out {pct_filtered:.2f}%)\")\n",
        "\n",
        "    # Same train/test split reused across experiments\n",
        "    train_data, test_data = train_test_split(filtered_data, test_size=0.2, random_state=42)\n",
        "\n",
        "    for loss_type in [\"kl\", \"hybrid\"]:\n",
        "        config_name = f\"cot_{use_cot}_loss_{loss_type}\"\n",
        "        evaluation, raw = run_distillation_experiment(\n",
        "            train_data=train_data,\n",
        "            test_data=test_data,\n",
        "            use_cot=use_cot,\n",
        "            loss_type=loss_type\n",
        "        )\n",
        "        evaluation_res[config_name] = evaluation\n",
        "        raw_res[config_name] = raw\n",
        "\n",
        "# Save results\n",
        "with open(\"all_distillation_evaluation.json\", \"w\") as f:\n",
        "    json.dump(evaluation_res, f, indent=2, default=str)\n",
        "\n",
        "with open(\"all_distillation_raw.json\", \"w\") as f:\n",
        "    json.dump(raw_res, f, indent=2, default=str)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "2\n2\n1\n1\n2\n1\n1\n1\n1\n1\n2\n1\n1\n2\n2\n2\n1\n1\n2\n1\n1\n2\n2\n2\n1\n2\n2\n2\n1\n1\n1\n2\n2\n2\n2\n1\n1\n2\n1\n2\n1\n2\n1\n1\n2\n2\n1\n2\n1\n2\n1\n1\n1\n1\n1\n2\n1\n1\n2\n2\n1\n1\n1\n2\n1\n1\n1\n1\n2\n1\n2\n2\n1\n2\n1\n2\n1\n1\n2\n2\n1\n2\n2\n2\n1\n1\n2\n2\n1\n2\n1\n1\n1\n2\n1\n2\n1\n1\n1\n1\n1\n1\n1\n1\n1\n2\n2\n1\n2\n2\n1\n1\n1\n1\n2\n1\n1\n2\n2\n1\n2\n1\n1\n1\n1\n2\n2\n1\n1\n2\n1\n1\n2\n1\n1\n1\n1\n1\n2\n2\n1\n1\n2\n1\n2\n2\n1\n1\n1\n2\n2\n2\n1\n2\n2\n1\n2\n1\n1\n1\n2\n1\n1\n1\n1\n1\n1\n2\n1\n1\n1\n1\n1\n1\n2\n1\n1\n2\n1\n2\n1\n2\n1\n2\n2\n1\n1\n1\n1\n2\n1\n1\n2\n2\n2\n1\n2\n2\n2\n1\n1\n1\n1\n2\n1\n2\n1\n2\n2\n1\n2\n2\n1\n1\n1\n2\n1\n1\n2\n2\n1\n2\n2\n1\n2\n1\n1\n2\n2\n2\n1\n1\n1\n1\n2\n2\n2\n1\n1\n1\n1\n2\n2\n1\n2\n2\n2\n1\n2\n1\n1\n1\n1\n1\n1\n1\n1\n2\n1\n1\n2\n1\n2\n1\n2\n1\n2\n1\n1\n1\n2\n2\n1\n1\n1\n1\n1\n1\n2\n1\n1\n1\n2\n1\n1\n1\n1\n2\n1\n2\n1\n1\n1\n1\n1\n1\n1\n2\n1\n1\n1\n1\n2\n2\n1\n2\n2\n1\n2\n1\n1\n2\n2\n2\n1\n2\n1\n2\n1\n2\n1\n1\n2\n2\n2\n2\n1\n2\n1\n2\n2\n2\n2\n2\n1\n1\n1\n2\n1\n2\n1\n2\n1\n2\n2\n1\n2\n1\n1\n1\n1\n2\n1\n1\n2\n1\n1\n2\n1\n2\n1\n2\n1\n1\n2\n1\n2\n2\n2\n2\n2\n1\n1\n2\n2\n1\n2\n1\n2\n1\n2\n1\n2\n1\n2\n1\n2\n1\n1\n1\n1\n2\n1\n2\n2\n2\n1\n1\n1\n1\n2\n2\n2\n2\n1\n2\n1\n1\n1\n1\n1\n1\n2\n2\n2\n1\n1\n1\n1\n2\n1\n1\n1\n1\n1\n1\n1\n2\n2\n1\n1\n2\n2\n1\n1\n2\n1\n2\n2\n1\n1\n1\n1\n2\n1\n2\n1\n2\n1\n1\n1\n2\n2\n1\n1\n1\n2\n2\n1\n2\n2\n2\n1\n1\n2\n1\n2\n2\n2\n2\n1\n1\n1\n1\n1\n2\n2\n1\n1\n2\n1\n2\n1\n2\n2\n2\n2\n1\n2\n2\n1\n1\n1\n2\n1\n1\n2\n2\n2\n2\n1\n2\n2\n1\n2\n2\n1\n1\n2\n1\n1\n2\n2\n1\n1\n1\n1\n1\n2\n1\n2\n1\n2\n1\n2\n1\n1\n1\n2\n1\n1\n1\n2\n1\n2\n1\n1\n1\n1\n1\n1\n1\n2\n1\n1\n1\n1\n2\n1\n2\n2\n1\n2\n2\n1\n2\n1\n2\n1\n1\n1\n1\n1\n1\n2\n2\n1\n2\n1\n2\n1\n2\n2\n1\n2\n2\n1\n2\n1\n1\n2\n1\n1\n2\n2\n1\n2\n1\n2\n2\n1\n1\n1\n1\n1\n2\n1\n2\n1\n1\n2\n1\n1\n1\n1\n2\n2\n2\n1\n2\n1\n2\n1\n1\n1\n1\n1\n2\n1\n2\n1\n1\n2\n1\n1\n2\n2\n1\n2\n1\n2\n2\n2\n1\n1\n2\n2\n1\n1\n1\n1\n1\n2\n1\n1\n2\n2\n2\n2\n1\n1\n2\n1\n1\n2\n2\n2\n1\n1\n2\n2\n1\n2\n1\n1\n2\n1\n2\n1\n1\n1\n1\n1\n1\n1\n1\n1\n2\n1\n1\n2\n2\n1\n2\n2\n2\n2\n1\n2\n1\n1\n1\n2\n2\n1\n1\n2\n1\n1\n1\n1\n1\n1\n1\n1\n2\n1\n1\n1\n2\n1\n1\n1\n2\n1\n2\n1\n1\n2\n1\n1\n1\n2\n1\n1\n1\n1\n1\n1\n2\n1\n1\n1\n1\n1\n2\n1\n2\n1\n1\n1\n1\n1\n1\n2\n2\n1\n2\n1\n2\n2\n2\n2\n1\n1\n1\n2\n2\n2\n1\n2\n2\n1\n1\n1\n1\n2\n1\n2\n2\n2\n1\n2\n2\n2\n1\n1\n2\n1\n2\n1\n1\n2\n1\n2\n1\n2\n2\n2\n2\n1\n1\n2\n1\n2\n2\n1\n1\n1\n2\n2\n2\n2\n2\n1\n1\n2\n2\n1\n1\n2\n1\n2\n1\n2\n2\n2\n2\n2\n1\n1\n2\n2\n2\n1\n1\n1\n1\n2\n2\n1\n2\n1\n1\n2\n1\n2\n1\n2\n1\n1\n1\n2\n1\n2\n2\n1\n1\n1\n2\n2\n2\n2\n2\n2\n2\n2\n2\n1\n2\n2\n2\n1\n2\n2\n1\n2\n2\n1\n2\n1\n1\n1\n1\n1\n1\n1\n1\n1\n2\n1\n1\n1\n1\n1\n1\n2\n1\n1\n2\n1\n1\n2\n1\n2\n2\n1\n2\n1\n2\n1\n1\n2\n1\n2\n1\n1\n1\n2\n2\n1\n1\n1\n2\n1\n1\n1\n1\n2\n2\n2\n2\n1\n1\n2\n2\n2\n1\n1\n2\n1\n2\n1\n2\n1\n2\n1\n1\n1\n1\n1\n1\n1\n1\n2\n1\n1\n2\n2\n2\n2\n1\n2\n1\n2\n1\n1\n2\n1\n2\n1\n1\n1\n1\n2\n1\n1\n1\n1\n1\n2\n1\n2\n2\n1\n1\n2\n1\n1\n2\n2\n1\n1\n1\n2\n1\n2\n1\n2\n2\n2\n1\n1\n1\n2\n2\n1\n2\n2\n1\n1\n2\n2\n1\n1\n1\n1\n1\n2\n1\n1\n1\n1\n2\n1\n1\n2\n1\n1\n2\n1\n2\n1\n2\n1\n2\n2\n2\n1\n2\n1\n2\n1\n1\n2\n1\n1\n2\n2\n2\n1\n2\n1\n1\n2\n1\n1\n2\n2\n1\n2\n2\n1\n1\n1\n1\n2\n2\n2\n2\n1\n1\n1\n1\n2\n2\n2\n1\n1\n1\n1\n2\n1\n2\n2\n2\n2\n1\n1\n2\n1\n1\n1\n1\n2\n2\n1\n1\n1\n2\n2\n1\n1\n2\n2\n1\n2\n1\n1\n2\n2\n2\n1\n1\n2\n1\n1\n1\n1\n1\n2\n1\n2\n1\n1\n1\n2\n1\n2\n1\n1\n1\n2\n1\n1\n2\n2\n1\n1\n2\n1\n1\n1\n1\n2\n2\n2\n1\n2\n2\n2\n2\n2\n1\n1\n2\n2\n2\n2\n1\n2\n2\n1\n2\n1\n2\n2\n1\n2\n1\n2\n2\n2\n2\n1\n1\n2\n2\n1\n1\n2\n2\n1\n2\n2\n1\n1\n1\n2\n1\n2\n2\n2\n2\n1\n2\n1\n1\n2\n1\n1\n2\n1\n1\n1\n2\n2\n1\n2\n2\n1\n2\n1\n1\n2\n1\n1\n2\n1\n1\n1\n1\n1\n1\n1\n2\n1\n1\n1\n1\n1\n1\n2\n2\n1\n2\n1\n1\n1\n2\n1\n1\n1\n2\n1\n2\n1\n2\n2\n2\n2\n1\n2\n1\n2\n1\n1\n2\n2\n1\n2\n1\n1\n2\n1\n1\n2\n1\n1\n1\n2\n2\n2\n1\n2\n1\n1\n2\n1\n2\n1\n1\n2\n1\n1\n1\n2\n1\n2\n1\n2\n1\n2\n2\n2\n2\n1\n2\n1\n1\n1\n2\n2\n1\n2\n1\n2\n2\n1\n1\n2\n2\n2\n2\n2\n1\n2\n2\n1\n1\n1\n2\n2\n2\n1\n2\n1\n2\n1\n1\n2\n1\n2\n1\n2\n2\n1\n1\n1\n1\n2\n2\n1\n2\n1\n1\n2\n1\n1\n1\n2\n2\n1\n1\n1\n1\n2\n2\n1\n1\n1\n2\n2\n1\n2\n1\n1\n1\n1\n2\n2\n2\n1\n2\n1\n1\n2\n2\n1\n1\n2\n1\n2\n2\n2\n1\n2\n2\n1\n1\n2\n2\n1\n1\n1\n2\n1\n2\n2\n2\n1\n1\n1\n2\n1\n1\n1\n2\n2\n1\n1\n2\n2\n2\n1\n1\n2\n2\n1\n2\n2\n1\n1\n2\n1\n2\n1\n1\n1\n1\n2\n1\n2\n1\n1\n1\n2\n2\n2\n1\n1\n2\n2\n2\n1\n2\n1\n2\n2\n2\n1\n1\n1\n2\n2\n1\n1\n1\n2\n1\n1\n2\n2\n1\n2\n1\n1\n1\n2\n1\n2\n1\n1\n1\n1\n2\n1\n2\n1\n1\n2\n2\n1\n1\n1\n1\n2\n1\n1\n2\n1\n2\n2\n1\n1\n2\n1\n2\n2\n2\n1\n2\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n2\n1\n1\n1\n2\n1\n2\n2\n1\n1\n1\n1\n1\n2\n1\n2\n1\n1\n2\n2\n1\n1\n2\n1\n2\n1\n2\n1\n2\n2\n2\n2\n2\n2\n1\n1\n2\n2\n1\n2\n1\n1\n2\n2\n1\n2\n1\n1\n2\n1\n2\n1\n2\n2\n2\n2\n2\n2\n2\n2\n2\n1\n2\n1\n1\n2\n2\n2\n1\n2\n1\n2\n1\n1\n2\n2\n2\n2\n1\n1\n1\n2\n2\n1\n1\n1\n1\n1\n1\n1\n1\n2\n2\n2\n1\n1\n2\n1\n2\n1\n1\n2\n1\n2\n2\n2\n1\n2\n2\n1\n2\n2\n2\n1\n2\n1\n1\n2\n1\n2\n2\n2\n2\n2\n2\n1\n2\n2\n1\n2\n2\n1\n1\n2\n1\n2\n1\n2\n1\n1\n1\n2\n1\n2\n1\n1\n1\n2\n1\n1\n2\n1\n2\n1\n1\n1\n2\n1\n1\n1\n2\n1\n1\n2\n1\n2\n2\n2\n2\n1\n1\n2\n1\n2\n1\n2\n1\n2\n1\n2\n1\n1\n1\n1\n1\n1\n2\n1\n1\n2\n2\n2\n1\n1\n2\n1\n2\n1\n2\n1\n1\n1\n1\n1\n1\n1\n2\n2\n1\n1\n1\n2\n1\n2\n2\n1\n2\n2\n2\n2\n2\n2\n1\n1\n1\n1\n2\n1\n1\n2\n1\n1\n1\n1\n2\n2\n2\n1\n1\n2\n1\n2\n1\n1\n1\n1\n2\n2\n1\n2\n1\n1\n2\n1\n2\n2\n1\n1\n1\n1\n1\n2\n1\n1\n1\n2\n2\n2\n2\n2\n1\n1\n2\n2\n1\n2\n1\n2\n2\n1\n1\n1\n1\n1\n1\n1\n1\n2\n2\n1\n2\n2\n2\n2\n2\n1\n1\n2\n1\n1\n1\n2\n2\n2\n1\n1\n1\n1\n1\n2\n1\n1\n1\n1\n1\n2\n2\n1\n1\n2\n2\n1\n1\n2\n2\n2\n1\n2\n2\n1\n2\n1\n2\n2\n1\n2\n2\n2\n2\n1\n2\n2\n2\n2\n2\n2\n2\n2\n2\n1\n2\n2\n1\n1\n2\n2\n1\n2\n2\n1\n2\n1\n1\n2\n1\n2\n2\n1\n2\n1\n1\n2\n2\n1\n2\n1\n2\n1\n2\n2\n1\n2\n1\n1\n1\n1\n1\n2\n1\n1\n2\n1\n2\n2\n2\n1\n2\n1\n2\n1\n1\n1\n1\n2\n2\n1\n1\n1\n1\n2\n1\n2\n2\n1\n1\n1\n2\n1\n1\n2\n1\n1\n1\n2\n2\n1\n1\n1\n1\n2\n2\n2\n1\n1\n2\n2\n2\n2\n1\n2\n1\n1\n2\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n2\n2\n2\n2\n1\n2\n1\n2\n2\n1\n1\n1\n1\n2\n1\n1\n1\n1\n1\n1\n1\n2\n2\n1\n2\n2\n1\n1\n2\n2\n2\n1\n1\n2\n2\n2\n1\n2\n1\n2\n1\n1\n1\n1\n2\n2\n1\n1\n2\n2\n1\n1\n1\n1\n2\n2\n1\n1\n2\n2\n1\n1\n2\n2\n1\n1\n1\n2\n1\n2\n2\n2\n1\n1\n2\n2\n1\n2\n1\n1\n1\n1\n1\n1\n2\n1\n2\n1\n1\n2\n1\n2\n2\n1\n1\n2\n2\n1\n1\n2\n1\n2\n2\n2\n1\n1\n1\n1\n1\n2\n1\n1\n2\n1\n1\n1\n2\n2\n2\n2\n1\n1\n1\n2\n2\n2\n1\n1\n1\n2\n1\n2\n1\n2\n2\n1\n2\n1\n2\n1\n2\n1\n1\n1\n2\n2\n2\n1\n2\n1\n1\n1\n2\n1\n2\n2\n2\n1\n2\n2\n2\n2\n2\n2\n1\n1\n1\n2\n1\n1\n1\n1\n2\n2\n2\n2\n2\n1\n2\n1\n2\n2\n1\n1\n1\n2\n1\n2\n1\n1\n2\n2\n2\n1\n2\n1\n1\n1\n1\n1\n2\n2\n1\n1\n2\n1\n2\n1\n2\n1\n2\n1\n2\n1\n1\n2\n1\n2\n2\n2\n1\n1\n1\n2\n2\n1\n1\n2\n1\n1\n2\n2\n1\n1\n1\n1\n2\n1\n1\n2\n1\n2\n1\n1\n1\n2\n2\n1\n2\n2\n1\n1\n1\n1\n2\n1\n2\n2\n2\n1\n1\n1\n1\n1\n2\n2\n1\n1\n1\n2\n1\n1\n2\n2\n1\n2\n2\n2\n2\n2\n1\n1\n1\n1\n2\n2\nNeither option aligns closely with the user's preferences for horror and darker themes. However, if I had to choose, I would say:\n\n2\n1\n2\n2\n1\n1\n2\n1\n1\n1\n1\n2\n2\n2\n2\n1\n2\n1\n1\n1\n1\n1\n2\n"
        }
      ],
      "execution_count": 35,
      "metadata": {
        "gather": {
          "logged": 1752454440293
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(1)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "1\n"
        }
      ],
      "execution_count": 24,
      "metadata": {
        "gather": {
          "logged": 1752453739142
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.18",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}