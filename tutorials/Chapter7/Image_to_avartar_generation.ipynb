{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Personalized Profile Generation from Book Reviews and Ratings\n",
        "\n",
        "##### Objectives\n",
        "\n",
        "- **Profile Generation:** Use a text generation model to produce a personalized profile summary for each user.\n",
        "- **Evaluation:** Assess the quality of generated profiles by measuring coherence, relevance, and personalization. Optionally, use an LLM-as-a-judge approach for evaluation.\n"
      ],
      "metadata": {},
      "id": "d0fa2638"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "import logging\n",
        "logging.getLogger(\"datasets\").setLevel(logging.ERROR)\n",
        "\n",
        "\n",
        "def get_user_aggregated_reviews(\n",
        "    dataset_str=\"McAuley-Lab/Amazon-Reviews-2023\",\n",
        "    review_key=\"raw_review_Books\",\n",
        "    meta_key=\"raw_meta_Books\",\n",
        "    sampling_percent=0.1,\n",
        "    min_reviews_per_user=5,\n",
        "    n_users=5000,\n",
        "    n_reviews_per_user=5,\n",
        "    random_state=42\n",
        "):\n",
        "    \"\"\"\n",
        "    Returns a dictionary mapping user_id to a list of up to n_reviews_per_user aggregated review dicts.\n",
        "    Each review dict contains title, genre, rating, review, and description.\n",
        "    \"\"\"\n",
        "    # Load reviews\n",
        "    # split_str = f\"full[:{sampling_percent}%]\"\n",
        "    split_str = \"full[:5000]\"\n",
        "    reviews = load_dataset(dataset_str, review_key, split=split_str, trust_remote_code=True)\n",
        "    reviews_df = pd.DataFrame(reviews)\n",
        "    \n",
        "    reviews_df = reviews_df[['user_id', 'parent_asin', 'timestamp', 'rating', 'text']]\n",
        "    reviews_df.rename(columns={'parent_asin': 'item_id'}, inplace=True)\n",
        "\n",
        "    # Load metadata\n",
        "    metadata_stream = load_dataset(dataset_str, meta_key, split=\"full\", streaming=True, trust_remote_code=True)\n",
        "\n",
        "    # Build a set of item_ids from reviews_df\n",
        "    item_ids_set = set(reviews_df['item_id'].unique())\n",
        "\n",
        "    filtered_metadata = []\n",
        "    for example in metadata_stream:\n",
        "        if example['parent_asin'] in item_ids_set:\n",
        "            filtered_metadata.append(example)\n",
        "\n",
        "    metadata_df = pd.DataFrame(filtered_metadata)\n",
        "    metadata_df = metadata_df[['parent_asin', 'title', 'main_category', 'categories', 'description']]\n",
        "    metadata_df.rename(columns={'parent_asin': 'item_id'}, inplace=True)\n",
        "    metadata_df['genre'] = metadata_df['main_category']\n",
        "\n",
        "    # Build metadata lookup for fast access\n",
        "    book_metadata = metadata_df.set_index('item_id').to_dict(orient='index')\n",
        "\n",
        "    # Filter users with at least min_reviews_per_user reviews\n",
        "    user_review_counts = reviews_df.groupby('user_id').size()\n",
        "    eligible_users = user_review_counts[user_review_counts >= min_reviews_per_user].index\n",
        "\n",
        "    # Sample n_users users\n",
        "    if len(eligible_users) > n_users:\n",
        "        selected_users = pd.Series(eligible_users).sample(n=n_users, random_state=random_state)\n",
        "    else:\n",
        "        selected_users = pd.Series(eligible_users)\n",
        "        print(f\"Warning: Only {len(selected_users)} users with at least {min_reviews_per_user} reviews.\")\n",
        "\n",
        "    # Filter reviews to selected users\n",
        "    filtered_reviews = reviews_df[reviews_df['user_id'].isin(selected_users)]\n",
        "\n",
        "    # Aggregate reviews per user\n",
        "    user_aggregated = {}\n",
        "    for user_id in selected_users:\n",
        "        user_reviews = filtered_reviews[filtered_reviews['user_id'] == user_id]\n",
        "        user_reviews = user_reviews.sort_values(by='timestamp', ascending=False).head(n_reviews_per_user)\n",
        "        aggregated = []\n",
        "        for _, row in user_reviews.iterrows():\n",
        "            meta = book_metadata.get(row['item_id'], {})\n",
        "            aggregated.append({\n",
        "                \"title\": meta.get(\"title\", \"\"),\n",
        "                \"genre\": meta.get(\"genre\", \"\"),\n",
        "                \"rating\": row[\"rating\"],\n",
        "                \"review\": row[\"text\"],\n",
        "                \"description\": meta.get(\"description\", \"\")\n",
        "            })\n",
        "        user_aggregated[user_id] = aggregated\n",
        "\n",
        "    return user_aggregated\n",
        "\n",
        "# Example usage:\n",
        "user_books_data = get_user_aggregated_reviews()\n",
        "print(f\"Total users with aggregated data: {len(user_books_data)}\")\n",
        "for i, (user_id, books) in enumerate(user_books_data.items()):\n",
        "    if i < 3:\n",
        "        print(f\"\\nUser ID: {user_id}\")\n",
        "        for book in books:\n",
        "            print(f\"  Title: {book['title']}\")\n",
        "            print(f\"  Genre: {book['genre']}\")\n",
        "            print(f\"  Rating: {book['rating']}\")\n",
        "            print(f\"  Review: {book['review'][:60]}...\")\n",
        "            print(f\"  Description: {book['description'][:60]}...\")\n",
        "            print(\"---\")\n"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1750734636765
        }
      },
      "id": "da7e0c44"
    },
    {
      "cell_type": "markdown",
      "source": [
        "{\n",
        "  \"title\": \"Thinking, Fast and Slow\",\n",
        "  \"genre\": \"Behavioral Economics\",\n",
        "  \"rating\": 5,\n",
        "  \"review\": \"Fascinating insights into human decision-making. Changed how I view choices and biases.\",\n",
        "  \"description\": \"A groundbreaking exploration of human cognition by Nobel laureate Daniel Kahneman.\"\n",
        "}\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "e1e268f4-2280-4a74-bb09-a5b39827eaf2"
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import openai\n",
        "import yaml\n",
        "\n",
        "# Read the YAML file\n",
        "with open('./../../../Curify/curify_api.yaml', 'r') as yaml_file:\n",
        "    data = yaml.safe_load(yaml_file)\n",
        "\n",
        "# Access the API keys and other configuration data\n",
        "api_key = data.get('openai').get('api_key')\n",
        "\n",
        "def generate_profile_with_llm(reviews_structured, prompt_template, model, tokenizer=None, api_key=None):\n",
        "    \"\"\"\n",
        "    Generate a user profile summary from structured reviews using specified LLM.\n",
        "    model: either 'gpt-4o' (OpenAI) or a HuggingFace pipeline/model (e.g., FLAN-T5).\n",
        "    tokenizer: required if using HuggingFace model.\n",
        "    \"\"\"\n",
        "    \n",
        "    input_str = json.dumps(reviews_structured, ensure_ascii=False, indent=2)\n",
        "    prompt = prompt_template.format(reviews=input_str)\n",
        "\n",
        "    if model == \"gpt-4o\":\n",
        "        openai.api_key = api_key\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.7\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    else:\n",
        "        # Assume HuggingFace pipeline or model/tokenizer\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "        outputs = model.generate(**inputs, max_new_tokens=128)\n",
        "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "PROFILE_PROMPT = (\n",
        "    \"Given the following books, reviews, and genres, summarize the user's interests, \"\n",
        "    \"personality traits, and reading preferences in 3â€“4 sentences.\\n\\n\"\n",
        "    \"{reviews}\"\n",
        ")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1750671308827
        }
      },
      "id": "f94eb051"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def genre_recall(profile_text, genres):\n",
        "    \"\"\"Compute recall: % of genres mentioned in profile.\"\"\"\n",
        "    found = sum(1 for genre in genres if genre.lower() in profile_text.lower())\n",
        "    return found / len(genres) if genres else 0.0\n",
        "\n",
        "def embedding_similarity(profile_text, reviews, embed_fn):\n",
        "    \"\"\"Cosine similarity between profile and concatenated reviews using embed_fn (e.g., OpenAI or SentenceTransformer).\"\"\"\n",
        "    texts = [profile_text, \" \".join(reviews)]\n",
        "    embeddings = embed_fn(texts)\n",
        "    return cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
        "\n",
        "def keyword_overlap(profile_text, reviews):\n",
        "    \"\"\"TF-IDF keyword overlap between profile and reviews.\"\"\"\n",
        "    corpus = [profile_text, \" \".join(reviews)]\n",
        "    vectorizer = TfidfVectorizer().fit(corpus)\n",
        "    tfidf = vectorizer.transform(corpus)\n",
        "    overlap = (tfidf[0].multiply(tfidf[1])).sum()\n",
        "    return overlap / tfidf[0].sum() if tfidf[0].sum() else 0.0\n",
        "\n",
        "\n",
        "def llm_as_judge(profile_text, reference_reviews, model=\"gpt-4o\", api_key=None):\n",
        "    \"\"\"\n",
        "    Ask GPT-4o to rate the profile for relevance, personalization, coherence, and provide comments.\n",
        "    \"\"\"\n",
        "    prompt = (\n",
        "        \"Given the following user reviews:\\n\"\n",
        "        f\"{reference_reviews}\\n\\n\"\n",
        "        \"And the following profile summary:\\n\"\n",
        "        f\"{profile_text}\\n\\n\"\n",
        "        \"Rate the profile on a scale of 1-5 for:\\n\"\n",
        "        \"- Relevance\\n- Personalization\\n- Coherence\\n\"\n",
        "        \"Also provide a short comment.\"\n",
        "    )\n",
        "    openai.api_key = api_key\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1750671308849
        }
      },
      "id": "76c4f2e7"
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment_for_user(user_reviews, book_metadata, prompt_template, gpt_api_key, flan_model, flan_tokenizer, embed_fn):\n",
        "    # Aggregate reviews\n",
        "    structured = get_user_aggregated_reviews(user_reviews, book_metadata, n_reviews=5)\n",
        "    genres = list({b.get(\"genre\", \"\") for b in structured if b.get(\"genre\")})\n",
        "\n",
        "    # Generate profiles\n",
        "    gpt_profile = generate_profile_with_llm(structured, prompt_template, \"gpt-4o\", api_key=gpt_api_key)\n",
        "    flan_profile = generate_profile_with_llm(structured, prompt_template, flan_model, tokenizer=flan_tokenizer)\n",
        "\n",
        "    # Evaluate\n",
        "    reviews_texts = [r['review'] for r in structured]\n",
        "    metrics = {\n",
        "        \"gpt-4o\": {\n",
        "            \"genre_recall\": genre_recall(gpt_profile, genres),\n",
        "            \"embedding_similarity\": embedding_similarity(gpt_profile, reviews_texts, embed_fn),\n",
        "            \"keyword_overlap\": keyword_overlap(gpt_profile, reviews_texts)\n",
        "        },\n",
        "        \"flan-t5\": {\n",
        "            \"genre_recall\": genre_recall(flan_profile, genres),\n",
        "            \"embedding_similarity\": embedding_similarity(flan_profile, reviews_texts, embed_fn),\n",
        "            \"keyword_overlap\": keyword_overlap(flan_profile, reviews_texts)\n",
        "        }\n",
        "    }\n",
        "    # LLM-as-a-judge\n",
        "    judge_gpt = llm_as_judge(gpt_profile, \" \".join(reviews_texts), model=\"gpt-4o\", api_key=gpt_api_key)\n",
        "    judge_flan = llm_as_judge(flan_profile, \" \".join(reviews_texts), model=\"gpt-4o\", api_key=gpt_api_key)\n",
        "\n",
        "    return {\n",
        "        \"gpt_profile\": gpt_profile,\n",
        "        \"flan_profile\": flan_profile,\n",
        "        \"metrics\": metrics,\n",
        "        \"judge_gpt\": judge_gpt,\n",
        "        \"judge_flan\": judge_flan\n",
        "    }\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1750671308873
        }
      },
      "id": "9431d15d-2373-43f6-83db-e47bca1a9d90"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}