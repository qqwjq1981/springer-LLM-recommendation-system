{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Tutorial: From Traditional to LLM-Based Recommendations Using MovieLens Dataset\n",
        "\n",
        "#### Objective\n",
        "- Transitioning from traditional recommendation methods to LLM-based approaches.\n",
        "- Challenges: Scalability, cold start problem, lack of context-awareness."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setting Up the Environment\n",
        "\n",
        "**Install Required Libraries**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %pip install numpy pandas scikit-learn matplotlib surprise transformers torch\n",
        "# %pip install rapidfuzz"
      ],
      "outputs": [],
      "execution_count": 34,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1742364254524
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ### Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from transformers import pipeline, AutoTokenizer, AutoModel\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import json\n",
        "import yaml\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "\n",
        "# Load API keys from YAML file\n",
        "with open('./../../../Curify/curify_api.yaml', 'r') as yaml_file:\n",
        "    data = yaml.safe_load(yaml_file)\n",
        "\n",
        "openai_api_key = data.get('openai').get('api_key')  # Assuming OpenAI API key is stored under 'openai'\n",
        "client = OpenAI(api_key=openai_api_key)"
      ],
      "outputs": [],
      "execution_count": 35,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1742364254716
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 1: Load MovieLens Dataset\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load MovieLens Dataset\n",
        "url = \"https://files.grouplens.org/datasets/movielens/ml-latest-small.zip\"\n",
        "movielens_zip = \"movielens.zip\"\n",
        "movielens_folder = \"./../Data/ml-1m/\"\n",
        "\n",
        "# Load the dataset\n",
        "ratings = pd.read_csv(f\"{movielens_folder}ratings.dat\", \n",
        "                      sep=\"::\", \n",
        "                      engine=\"python\", \n",
        "                      names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n",
        "\n",
        "movies = pd.read_csv(f\"{movielens_folder}movies.dat\", \n",
        "                     sep=\"::\", \n",
        "                     engine=\"python\", \n",
        "                     encoding=\"ISO-8859-1\", \n",
        "                     header=None, \n",
        "                     names=[\"movieId\", \"title\", \"genres\"])\n",
        "\n",
        "# Create title-to-ID lookup dictionary\n",
        "movie_lookup = movies.set_index('title')['movieId'].to_dict()\n",
        "\n",
        "# Merge ratings with movie titles\n",
        "ratings = ratings.merge(movies, on=\"movieId\")\n",
        "\n",
        "# Convert timestamp to datetime for easier handling\n",
        "ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit='s')\n",
        "\n",
        "# ====== Temporal Split (Recommended) ======\n",
        "# Use a fixed percentile (e.g., 80%) as cutoff\n",
        "cutoff_time = ratings['timestamp'].quantile(0.9)\n",
        "\n",
        "# Split by timestamp\n",
        "train_data = ratings[ratings['timestamp'] <= cutoff_time].copy()\n",
        "test_data = ratings[ratings['timestamp'] > cutoff_time].copy()\n",
        "\n",
        "# Optionally filter test users that also exist in training set\n",
        "test_data = test_data[test_data['userId'].isin(train_data['userId'].unique())]\n",
        "\n",
        "# Optional: reset index\n",
        "train_data.reset_index(drop=True, inplace=True)\n",
        "test_data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# ====== Summary Statistics (optional) ======\n",
        "print(\"Train samples:\", len(train_data))\n",
        "print(\"Test samples :\", len(test_data))\n",
        "print(\"Time cutoff  :\", cutoff_time)\n",
        "print(\"Number of test users  :\", len(test_data['userId'].unique()))\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Train samples: 900188\nTest samples : 95812\nTime cutoff  : 2000-12-29 23:42:56.400000\nNumber of test users  : 1180\n"
        }
      ],
      "execution_count": 36,
      "metadata": {
        "gather": {
          "logged": 1742364259452
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data['userId'].value_counts()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 37,
          "data": {
            "text/plain": "userId\n1088    1014\n1447     985\n678      936\n424      885\n531      860\n        ... \n997        1\n2583       1\n2330       1\n2505       1\n2059       1\nName: count, Length: 1180, dtype: int64"
          },
          "metadata": {}
        }
      ],
      "execution_count": 37,
      "metadata": {
        "gather": {
          "logged": 1742364259569
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 2: Traditional Model (Collaborative Filtering and Hybrid)\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from surprise import Dataset, Reader, SVD\n",
        "import pandas as pd\n",
        "\n",
        "# Train SVD model\n",
        "reader = Reader(rating_scale=(1, 5))\n",
        "data = Dataset.load_from_df(train_data[['userId', 'movieId', 'rating']], reader)\n",
        "trainset = data.build_full_trainset()\n",
        "model = SVD()\n",
        "model.fit(trainset)\n",
        "\n",
        "def traditional_recommendation(train_data, user_id, top_k=5, use_genres=False):\n",
        "    \"\"\"\n",
        "    Optimized traditional recommendation using collaborative filtering (SVD).\n",
        "    Efficiently filters unseen movies and returns top-k recommendations.\n",
        "\n",
        "    Args:\n",
        "        train_data (pd.DataFrame): ['userId', 'movieId', 'rating', 'genres'].\n",
        "        user_id (int): Target user.\n",
        "        top_k (int): Number of recommendations.\n",
        "        use_genres (bool): Placeholder flag (not used in this version).\n",
        "\n",
        "    Returns:\n",
        "        list: Top-k recommended movie IDs.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get all movie IDs not rated by the user\n",
        "    rated_movies = train_data[train_data['userId'] == user_id]['movieId'].unique()\n",
        "    all_movies = train_data['movieId'].unique()\n",
        "    unseen_movies = list(set(all_movies) - set(rated_movies))\n",
        "\n",
        "    # Predict ratings only for unseen movies\n",
        "    predictions = [(movie_id, model.predict(user_id, movie_id).est) for movie_id in unseen_movies]\n",
        "\n",
        "    # Get top-k movie IDs\n",
        "    top_k_movies = sorted(predictions, key=lambda x: x[1], reverse=True)[:top_k]\n",
        "    return [movie_id for movie_id, _ in top_k_movies]"
      ],
      "outputs": [],
      "execution_count": 38,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1742364272242
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 3: LLM-Based Model (IDs Only and IDs with Genres)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from rapidfuzz import process\n",
        "\n",
        "# Build title-to-ID dictionary\n",
        "movie_lookup = movies.set_index('title')['movieId'].to_dict()\n",
        "\n",
        "# Fuzzy matching function\n",
        "def map_title_to_id(title, threshold=80):\n",
        "    match, score, _ = process.extractOne(title, movie_lookup.keys())\n",
        "    return movie_lookup[match] if score >= threshold else None\n",
        "\n",
        "def llm_recommendation(train_data, user_id, top_k=5, openai_api_key=None, model_name=\"gpt-4o-mini\"):\n",
        "    \"\"\"\n",
        "    LLM-based recommendation using GPT-4o-mini API with structured prompts.\n",
        "    \n",
        "    Args:\n",
        "        train_data (pd.DataFrame): DataFrame with ['userId', 'movieId', 'title', 'rating'].\n",
        "        user_id (int): ID of the target user.\n",
        "        top_k (int): Number of recommendations.\n",
        "        openai_api_key (str): Your OpenAI API key.\n",
        "        model_name (str): Model name (default: gpt-4o-mini).\n",
        "    \n",
        "    Returns:\n",
        "        list: Top-k recommended movie IDs.\n",
        "    \"\"\"\n",
        "    openai.api_key = openai_api_key\n",
        "\n",
        "    # Filter user's ratings\n",
        "    user_ratings = train_data[train_data['userId'] == user_id]\n",
        "    liked = user_ratings[user_ratings['rating'] >= 4].nlargest(10, 'rating')\n",
        "    disliked = user_ratings[user_ratings['rating'] <= 2].nsmallest(10, 'rating')\n",
        "\n",
        "    # Get movie titles\n",
        "    liked_titles = [row['title'] for _, row in liked.iterrows()]\n",
        "    disliked_titles = [row['title'] for _, row in disliked.iterrows()]\n",
        "\n",
        "    # Construct prompt with role-play\n",
        "    prompt = (\n",
        "        f\"You are a helpful movie recommendation assistant.\\n\"\n",
        "        f\"The user USER_{user_id} liked the following movies: {', '.join(liked_titles)}.\\n\"\n",
        "        f\"The user USER_{user_id} disliked these movies: {', '.join(disliked_titles)}.\\n\"\n",
        "        f\"Please recommend exactly {top_k} movies that are similar to the liked ones and different from the disliked ones.\\n\"\n",
        "        f\"Output only the recommended movie titles, sorted by their relevance to the user's preferences, separated by commas.\"\n",
        "    )\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.9,\n",
        "        max_tokens=1024\n",
        "    )\n",
        "    # print(prompt)\n",
        "    # Extract and parse output\n",
        "    generated_text = response.choices[0].message.content.strip()\n",
        "    recommended_titles = [title.strip() for title in generated_text.split(\",\")]\n",
        "    # print(generated_text)\n",
        "    # Map recommended titles to movie IDs\n",
        "    recommended_ids = [map_title_to_id(title) for title in recommended_titles]\n",
        "    return [movie_id for movie_id in recommended_ids if movie_id is not None]\n"
      ],
      "outputs": [],
      "execution_count": 39,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1742364272365
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 4: Evaluating Model Performance"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_recommendations(train_data, test_data, model_type=\"traditional\", use_genres=False, top_k=5):\n",
        "    \"\"\"\n",
        "    Generate recommendations for all users in the test data.\n",
        "    \n",
        "    Args:\n",
        "        train_data (pd.DataFrame): Training data with columns ['userId', 'movieId', 'rating', 'genres'].\n",
        "        test_data (pd.DataFrame): Test data with columns ['userId', 'movieId', 'rating', 'genres'].\n",
        "        model_type (str): Type of model to use (\"traditional\" or \"llm\").\n",
        "        use_genres (bool): Whether to use genres in the model.\n",
        "        top_k (int): Number of recommendations to generate.\n",
        "    \n",
        "    Returns:\n",
        "        dict: A dictionary mapping user IDs to their top-k recommendations.\n",
        "    \"\"\"\n",
        "    user_recommendations = {}\n",
        "    unique_users = test_data['userId'].unique()\n",
        "    \n",
        "    for user_id in unique_users:\n",
        "        if model_type == \"traditional\":\n",
        "            recommendations = traditional_recommendation(train_data, user_id, top_k, use_genres)\n",
        "        elif model_type == \"llm\":\n",
        "            recommendations = llm_recommendation(train_data, user_id, top_k, use_genres)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid model_type. Choose 'traditional' or 'llm'.\")\n",
        "        \n",
        "        user_recommendations[user_id] = recommendations\n",
        "    \n",
        "    return user_recommendations"
      ],
      "outputs": [],
      "execution_count": 40,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1742364272594
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ndcg_score\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "def evaluate_recommendations(test_data, user_recommendations, top_k=5):\n",
        "    \"\"\"\n",
        "    Evaluate recommendations using Recall@k, Precision@k, NDCG@k,\n",
        "    Effective Catalog Ratio, and Entropy-based Diversity.\n",
        "    \n",
        "    Args:\n",
        "        test_data (pd.DataFrame): Test data with ['userId', 'movieId', 'rating'].\n",
        "        user_recommendations (dict): {userId: [recommended_movieIds]}\n",
        "        movies (pd.DataFrame): Movie catalog with 'movieId'.\n",
        "        top_k (int): Number of recommendations per user.\n",
        "    \n",
        "    Returns:\n",
        "        dict: Evaluation metrics.\n",
        "    \"\"\"\n",
        "    recall_scores = []\n",
        "    precision_scores = []\n",
        "    ndcg_scores = []\n",
        "\n",
        "    all_recommended_items = []\n",
        "\n",
        "    for user_id, recommendations in user_recommendations.items():\n",
        "        # Ground truth positives\n",
        "        user_positive = test_data[\n",
        "            (test_data['userId'] == user_id) & (test_data['rating'] >= 4)\n",
        "        ]['movieId'].tolist()\n",
        "\n",
        "        if len(user_positive) == 0:\n",
        "            continue\n",
        "\n",
        "        # Accumulate recommended items for diversity metrics\n",
        "        all_recommended_items.extend(recommendations)\n",
        "\n",
        "        # Recall and Precision\n",
        "        relevant = len(set(recommendations) & set(user_positive))\n",
        "        recall_scores.append(relevant / len(user_positive))\n",
        "        precision_scores.append(relevant / top_k)\n",
        "\n",
        "        # NDCG\n",
        "        relevance = [1 if movie_id in user_positive else 0 for movie_id in recommendations]\n",
        "        # Pad relevance vector if fewer than top_k\n",
        "        if len(relevance) < top_k:\n",
        "            relevance += [0] * (top_k - len(relevance))\n",
        "\n",
        "        # Simulated predicted scores from top_k to 1\n",
        "        predicted_scores = list(range(top_k, 0, -1))  # simulate rank positions as scores\n",
        "        try:\n",
        "            ndcg = ndcg_score([relevance], [predicted_scores], k=len(relevance))\n",
        "            ndcg_scores.append(ndcg)\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping NDCG for user {user_id}: {e}\")\n",
        "\n",
        "    # Catalog Coverage Ratio\n",
        "    unique_recommended = len(set(all_recommended_items))\n",
        "    total_catalog_size = movies['movieId'].nunique()\n",
        "    catalog_coverage_ratio = unique_recommended / total_catalog_size\n",
        "\n",
        "    # Entropy-based Diversity\n",
        "    item_freq = Counter(all_recommended_items)\n",
        "    total_recs = sum(item_freq.values())\n",
        "    entropy = -sum((freq / total_recs) * math.log2(freq / total_recs) for freq in item_freq.values())\n",
        "    normalized_entropy = entropy / math.log2(total_catalog_size) if total_catalog_size > 0 else 0\n",
        "\n",
        "    # Final Metrics\n",
        "    if len(recall_scores) == 0:\n",
        "        return {\n",
        "            \"Recall@k\": 0.0,\n",
        "            \"Precision@k\": 0.0,\n",
        "            \"NDCG@k\": 0.0,\n",
        "            \"Catalog Coverage Ratio\": 0.0,\n",
        "            \"Entropy Diversity\": 0.0,\n",
        "            \"Total users\": 0\n",
        "        }\n",
        "\n",
        "    metrics = {\n",
        "        \"Recall@k\": np.mean(recall_scores),\n",
        "        \"Precision@k\": np.mean(precision_scores),\n",
        "        \"NDCG@k\": np.mean(ndcg_scores),\n",
        "        \"Catalog Coverage Ratio\": catalog_coverage_ratio,\n",
        "        \"Entropy Diversity\": normalized_entropy,\n",
        "        \"Total users\": len(user_recommendations.items())\n",
        "    }\n",
        "\n",
        "    return metrics\n"
      ],
      "outputs": [],
      "execution_count": 41,
      "metadata": {
        "gather": {
          "logged": 1742364272692
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from sklearn.metrics import ndcg_score\n",
        "import numpy as np\n",
        "\n",
        "# Function to evaluate all four methods\n",
        "def evaluate_all_methods(train_data, test_data, top_k=5):\n",
        "    \"\"\"\n",
        "    Evaluate all four recommendation methods and time the entire operation.\n",
        "    \n",
        "    Args:\n",
        "        train_data (pd.DataFrame): Training data with columns ['userId', 'movieId', 'rating', 'genres'].\n",
        "        test_data (pd.DataFrame): Test data with columns ['userId', 'movieId', 'rating', 'genres'].\n",
        "        top_k (int): Number of recommendations to evaluate.\n",
        "    \n",
        "    Returns:\n",
        "        dict: A dictionary containing evaluation metrics and execution times for all methods.\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    \n",
        "    # Method 1: Traditional CF (User and Item IDs Only)\n",
        "    start_time = time.time()\n",
        "    user_recommendations = generate_recommendations(train_data, test_data, model_type=\"traditional\", use_genres=False, top_k=top_k)\n",
        "    metrics = evaluate_recommendations(test_data, user_recommendations, top_k=top_k)\n",
        "    execution_time = time.time() - start_time\n",
        "    results[\"traditional_cf\"] = {\"metrics\": metrics, \"execution_time\": execution_time}\n",
        "    \n",
        "    # Method 2: LLM Prompt with titles Only\n",
        "    start_time = time.time()\n",
        "    user_recommendations = generate_recommendations(train_data, test_data, model_type=\"llm\", use_genres=False, top_k=top_k)\n",
        "    metrics = evaluate_recommendations(test_data, user_recommendations, top_k=top_k)\n",
        "    execution_time = time.time() - start_time\n",
        "    results[\"llm_titles_only\"] = {\"metrics\": metrics, \"execution_time\": execution_time}\n",
        "    \n",
        "    return results"
      ],
      "outputs": [],
      "execution_count": 42,
      "metadata": {
        "gather": {
          "logged": 1742364272803
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# test_data = test_data[test_data['userId'].isin([1088, 1447,678, 531, 424, 997, 2583, 2330, 2505, 2059])]  # Filter test data for specific users\n",
        "\n",
        "results_5 = evaluate_all_methods(train_data, test_data, top_k=5)\n",
        "results_10 = evaluate_all_methods(train_data, test_data, top_k=10)\n",
        "results_20 = evaluate_all_methods(train_data, test_data, top_k=20)\n",
        "\n",
        "# Convert metrics and execution time to serializable format\n",
        "def serialize_results(results_dict):\n",
        "    serializable = {}\n",
        "    for method, result in results_dict.items():\n",
        "        serializable[method] = {\n",
        "            \"metrics\": {k: float(v) for k, v in result[\"metrics\"].items()},\n",
        "            \"execution_time\": float(result[\"execution_time\"])\n",
        "        }\n",
        "    return serializable\n",
        "\n",
        "# Serialize both results\n",
        "results_combined = {\n",
        "    \"results_top_5\": serialize_results(results_5),\n",
        "    \"results_top_10\": serialize_results(results_10),\n",
        "    \"results_top_20\": serialize_results(results_20)\n",
        "}\n",
        "\n",
        "# Save to JSON file\n",
        "with open(\"recommendation_results.json\", \"w\") as f:\n",
        "    json.dump(results_combined, f, indent=4)\n",
        "\n",
        "print(\"Saved results to recommendation_results.json\")\n",
        "\n",
        "results_combined"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Skipping NDCG for user 1447: array is not broadcastable to correct shape\nSaved results to recommendation_results.json\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 43,
          "data": {
            "text/plain": "{'results_top_5': {'traditional_cf': {'metrics': {'Recall@k': 0.002812690210883351,\n    'Precision@k': 0.275,\n    'NDCG@k': 0.3756426636604775,\n    'Catalog Coverage Ratio': 0.007468452227659027,\n    'Entropy Diversity': 0.3928710063979196,\n    'Total users': 10.0},\n   'execution_time': 0.2215566635131836},\n  'llm_titles_only': {'metrics': {'Recall@k': 0.0017093730935911914,\n    'Precision@k': 0.15000000000000002,\n    'NDCG@k': 0.35081236846854846,\n    'Catalog Coverage Ratio': 0.004893123873293845,\n    'Entropy Diversity': 0.3189637876362132,\n    'Total users': 10.0},\n   'execution_time': 9.944148063659668}},\n 'results_top_10': {'traditional_cf': {'metrics': {'Recall@k': 0.006710938339513937,\n    'Precision@k': 0.3125,\n    'NDCG@k': 0.41960966989938564,\n    'Catalog Coverage Ratio': 0.013391707442698944,\n    'Entropy Diversity': 0.4538098552229504,\n    'Total users': 10.0},\n   'execution_time': 0.21613454818725586},\n  'llm_titles_only': {'metrics': {'Recall@k': 0.12675740854765627,\n    'Precision@k': 0.1,\n    'NDCG@k': 0.2694511886619589,\n    'Catalog Coverage Ratio': 0.010816379088333763,\n    'Entropy Diversity': 0.4199252200575608,\n    'Total users': 10.0},\n   'execution_time': 19.5988552570343}},\n 'results_top_20': {'traditional_cf': {'metrics': {'Recall@k': 0.013991973207969086,\n    'Precision@k': 0.325,\n    'NDCG@k': 0.4399415730365048,\n    'Catalog Coverage Ratio': 0.02343548802472315,\n    'Entropy Diversity': 0.5205760506407906,\n    'Total users': 10.0},\n   'execution_time': 0.22017765045166016},\n  'llm_titles_only': {'metrics': {'Recall@k': 0.13001621198521265,\n    'Precision@k': 0.11875,\n    'NDCG@k': 0.46689498319202716,\n    'Catalog Coverage Ratio': 0.020087561164048418,\n    'Entropy Diversity': 0.4663219614938154,\n    'Total users': 10.0},\n   'execution_time': 27.418247938156128}}}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 43,
      "metadata": {
        "gather": {
          "logged": 1742364330065
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(1)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "1\n"
        }
      ],
      "execution_count": 56,
      "metadata": {
        "gather": {
          "logged": 1742372042393
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}